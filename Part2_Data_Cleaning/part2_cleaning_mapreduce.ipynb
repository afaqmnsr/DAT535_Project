{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Data Cleaning with MapReduce (Silver Layer)\n",
    "\n",
    "## Overview\n",
    "This notebook implements the **Silver Layer** of the Medallion Architecture, transforming raw data from the Bronze layer into cleaned, standardized, and validated data using **MapReduce/RDD operations exclusively**.\n",
    "\n",
    "## CRITICAL REQUIREMENT\n",
    "**This part MUST use MapReduce/RDD operations ONLY. NO SQL, NO DataFrames (except final conversion for inspection).**\n",
    "\n",
    "## Objectives\n",
    "1. **Data Parsing & Validation**: Parse raw JSON Lines and validate data quality using MapReduce patterns\n",
    "2. **Data Cleaning**: Standardize formats, normalize values, and filter invalid records\n",
    "3. **Deduplication**: Remove duplicate records using MapReduce `reduceByKey` operations\n",
    "4. **Data Quality Validation**: Comprehensive quality checks using MapReduce aggregations\n",
    "5. **Performance Profiling**: Measure and analyze execution times and throughput\n",
    "6. **Optimization & Tuning**: Identify bottlenecks and apply optimization strategies\n",
    "\n",
    "## Medallion Architecture: Silver Layer\n",
    "- **Input**: Raw JSON Lines from Bronze layer (`Part1_Data_Ingestion/raw_data/`)\n",
    "- **Processing**: MapReduce-based cleaning, validation, and deduplication\n",
    "- **Output**: Cleaned data in JSON Lines format (`silver_data/`) and DataFrame for inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/24 20:59:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java at: /usr/lib/jvm/java-17-openjdk-amd64/bin/java\n",
      "JAVA_HOME: /usr/lib/jvm/java-17-openjdk-amd64\n",
      "Spark version: 4.0.1 Default parallelism: 4\n"
     ]
    }
   ],
   "source": [
    "# Setup and Imports\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "from datetime import datetime  # <-- added\n",
    "\n",
    "# ---- Spark/JDK wiring (Notebook-only) ----\n",
    "import os, sys, subprocess, shutil\n",
    "\n",
    "# Point Spark to Java 17\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.path.join(os.environ[\"JAVA_HOME\"], \"bin\") + os.pathsep + os.environ.get(\"PATH\", \"\")\n",
    "\n",
    "# Make sure no conflicting env is set\n",
    "os.environ.pop(\"SPARK_HOME\", None)\n",
    "os.environ.pop(\"PYSPARK_SUBMIT_ARGS\", None)\n",
    "\n",
    "# Ensure pyspark is available in THIS kernel\n",
    "try:\n",
    "    import pyspark  # noqa\n",
    "except ModuleNotFoundError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--user\", \"pyspark==3.5.1\"])\n",
    "    import pyspark  # noqa\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session with memory configuration\n",
    "# Note: Memory settings are optional\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"Part2-DataCleaning-MapReduce\")\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "        .config(\"spark.executor.memory\", \"2g\")\n",
    "        .config(\"spark.driver.memory\", \"2g\")\n",
    "        .config(\"spark.executor.memoryFraction\", \"0.8\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"java at:\", shutil.which(\"java\"))\n",
    "print(\"JAVA_HOME:\", os.environ[\"JAVA_HOME\"])\n",
    "print(\"Spark version:\", spark.version, \"Default parallelism:\", sc.defaultParallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Raw Data from Bronze Layer\n",
    "\n",
    "Load the raw JSON Lines data ingested in Part 1. This data contains energy consumption records with ingestion metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw data\n",
      "Initial partition count: 64\n",
      "Sample records (first 3):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Record 1: {\"datetime\": \"2023-11-01 08:00:00\", \"Usage (kW)\": \"0.33\", \"city\": \"Islamabad\", \"source_file\": \"islamabad_House41.csv\", \"_ingestion_metadata\": {\"ingest...\n",
      "  Record 2: {\"datetime\": \"2023-11-01 08:01:00\", \"Usage (kW)\": \"0.01\", \"city\": \"Islamabad\", \"source_file\": \"islamabad_House41.csv\", \"_ingestion_metadata\": {\"ingest...\n",
      "  Record 3: {\"datetime\": \"2023-11-01 08:02:00\", \"Usage (kW)\": \"0.1\", \"city\": \"Islamabad\", \"source_file\": \"islamabad_House41.csv\", \"_ingestion_metadata\": {\"ingesti...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Load raw data from Bronze layer\n",
    "bronze_data_path = \"../Part1_Data_Ingestion/raw_data/\"\n",
    "\n",
    "# Load JSON lines (exclude metadata.json)\n",
    "raw_rdd = sc.textFile(f\"{bronze_data_path}/*.jsonl\").repartition(64)\n",
    "\n",
    "print(\"Loaded raw data\")\n",
    "print(\"Initial partition count:\", raw_rdd.getNumPartitions())\n",
    "print(\"Sample records (first 3):\")\n",
    "for i, line in enumerate(raw_rdd.take(3), 1):\n",
    "    print(f\"  Record {i}: {line[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Parsing and Validation (Map Phase)\n",
    "\n",
    "Parse raw JSON Lines records and validate data quality using MapReduce operations. This step includes:\n",
    "- **JSON Parsing**: Extract structured data from JSON Lines format\n",
    "- **Datetime Normalization**: Standardize timestamps to 'YYYY-MM-DD HH:MM:SS' format\n",
    "- **Type Conversion**: Convert consumption values from string to float\n",
    "- **Range Validation**: Ensure usage values are within valid range (0-50 kW)\n",
    "- **Field Standardization**: Trim and normalize text fields (city, source_file)\n",
    "- **Error Handling**: Categorize records as valid, invalid, or parse errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing complete\n",
      "Total records: 35734531\n",
      "Valid records: 33332692 (93.3%)\n",
      "Invalid records: 2401839 (6.7%)\n",
      "Skipped records: 0\n",
      "Error records: 0\n",
      "\n",
      "Sample invalid records:\n",
      "  Invalid record 1:\n",
      "    Errors: ['Consumption not numeric: NA']\n",
      "  Invalid record 2:\n",
      "    Errors: ['Consumption not numeric: NA']\n",
      "✓ Cleaned RDD created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:========================================================>(63 + 1) / 64]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid records available: 33,332,692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Define parsing function for REWDP JSON Lines -> returns {'status', 'data', 'errors'}\n",
    "def _normalize_dt(dt_val: Any) -> Optional[str]:\n",
    "    \"\"\"Return 'YYYY-MM-DD HH:MM:SS' or None.\"\"\"\n",
    "    if dt_val is None:\n",
    "        return None\n",
    "    s = str(dt_val).strip()\n",
    "    # try full \"YYYY-MM-DD HH:MM:SS\"\n",
    "    try:\n",
    "        if len(s) > 10:\n",
    "            return datetime.strptime(s[:19], \"%Y-%m-%d %H:%M:%S\").strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        # date-only\n",
    "        return datetime.strptime(s[:10], \"%Y-%m-%d\").strftime(\"%Y-%m-%d 00:00:00\")\n",
    "    except ValueError:\n",
    "        # Try ISO variants\n",
    "        try:\n",
    "            return datetime.fromisoformat(s.replace(\"Z\", \"+00:00\")[:19]).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def parse_and_validate(record: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse and validate raw record from Bronze layer (JSON Lines format).\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with:\n",
    "        - 'status': 'valid', 'invalid', 'parse_error', 'error', or 'skip'\n",
    "        - 'data': For valid records, tuple of (timestamp_str, usage_kw_float, city_str, source_file_str)\n",
    "                  For invalid/error records, None\n",
    "        - 'errors': List of error messages (empty for valid records)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = json.loads(record)\n",
    "\n",
    "        # Skip pure metadata records\n",
    "        if '_ingestion_metadata' in data and len(data) == 1:\n",
    "            return {'data': None, 'status': 'skip', 'errors': ['Metadata record']}\n",
    "\n",
    "        # Required fields\n",
    "        dt_raw = data.get('datetime') or data.get('timestamp')\n",
    "        dt_norm = _normalize_dt(dt_raw)\n",
    "        if not dt_norm:\n",
    "            return {'data': None, 'status': 'invalid', 'errors': [f\"Invalid/missing datetime: {dt_raw}\"]}\n",
    "\n",
    "        # Consumption field (string or number)\n",
    "        consumption_field = None\n",
    "        for field in ['Usage (kW)', 'Usage', 'consumption']:\n",
    "            if field in data:\n",
    "                consumption_field = field\n",
    "                break\n",
    "        if not consumption_field:\n",
    "            return {'data': None, 'status': 'invalid', 'errors': ['Missing consumption field']}\n",
    "\n",
    "        try:\n",
    "            usage = float(data[consumption_field])\n",
    "        except (ValueError, TypeError):\n",
    "            return {'data': None, 'status': 'invalid', 'errors': [f\"Consumption not numeric: {data.get(consumption_field)}\"]}\n",
    "\n",
    "        if usage < 0 or usage > 50:\n",
    "            return {'data': None, 'status': 'invalid', 'errors': [f\"Invalid consumption: {usage}\"]}\n",
    "\n",
    "        # Light payload (tuple) for valid rows\n",
    "        city = (data.get('city') or '').strip()\n",
    "        src  = (data.get('source_file') or '').strip()\n",
    "        return {'data': (dt_norm, usage, city, src), 'status': 'valid', 'errors': []}\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        return {'data': None, 'status': 'parse_error', 'errors': [str(e)]}\n",
    "    except Exception as e:\n",
    "        return {'data': None, 'status': 'error', 'errors': [str(e)]}\n",
    "\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "from operator import add\n",
    "\n",
    "# Map phase\n",
    "parsed_rdd = raw_rdd.map(parse_and_validate)\n",
    "\n",
    "# Spill to disk if needed (avoid huge memory pressure)\n",
    "parsed_rdd = parsed_rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Single pass status aggregation (Map -> ReduceByKey)\n",
    "status_counts = (parsed_rdd\n",
    "    .map(lambda x: (x['status'], 1))\n",
    "    .reduceByKey(add)\n",
    "    .collectAsMap())\n",
    "\n",
    "total_count  = sum(status_counts.values())\n",
    "valid_count  = status_counts.get('valid', 0)\n",
    "invalid_count = status_counts.get('invalid', 0)\n",
    "skip_count    = status_counts.get('skip', 0)\n",
    "error_count   = status_counts.get('parse_error', 0) + status_counts.get('error', 0)\n",
    "\n",
    "print(\"Parsing complete\")\n",
    "print(f\"Total records: {total_count}\")\n",
    "print(f\"Valid records: {valid_count} ({(valid_count/total_count*100 if total_count else 0):.1f}%)\")\n",
    "print(f\"Invalid records: {invalid_count} ({(invalid_count/total_count*100 if total_count else 0):.1f}%)\")\n",
    "print(f\"Skipped records: {skip_count}\")\n",
    "print(f\"Error records: {error_count}\")\n",
    "\n",
    "# Display sample invalid records for validation\n",
    "if invalid_count > 0:\n",
    "    print(\"\\nSample invalid records:\")\n",
    "    invalid_samples = parsed_rdd.filter(lambda x: x['status']=='invalid').take(2)\n",
    "    for i, sample in enumerate(invalid_samples, 1):\n",
    "        print(f\"  Invalid record {i}:\")\n",
    "        print(f\"    Errors: {sample['errors']}\")\n",
    "\n",
    "\n",
    "# Extract valid records as lightweight tuples for further processing\n",
    "clean_rdd = (parsed_rdd\n",
    "    .filter(lambda x: x['status'] == 'valid')\n",
    "    .map(lambda x: x['data']))\n",
    "\n",
    "print(\"✓ Cleaned RDD created\")\n",
    "print(f\"Valid records available: {clean_rdd.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=====================================================>  (61 + 3) / 64]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ No parse errors or exceptions found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Additional validation: Check for parse errors or exceptions\n",
    "error_count = parsed_rdd.filter(lambda x: x['status'] in ['parse_error', 'error']).count()\n",
    "if error_count > 0:\n",
    "    print(\"=== Error Record Analysis ===\")\n",
    "    error_samples = parsed_rdd.filter(lambda x: x['status'] in ['parse_error', 'error']).take(3)\n",
    "    for i, sample in enumerate(error_samples, 1):\n",
    "        print(f\"\\nError record {i}:\")\n",
    "        print(f\"  Status: {sample['status']}\")\n",
    "        print(f\"  Errors: {sample['errors']}\")\n",
    "        if 'raw' in sample.get('data', {}):\n",
    "            raw_data = sample['data']['raw']\n",
    "            print(f\"  Raw data (first 200 chars): {str(raw_data)[:200]}\")\n",
    "        elif isinstance(sample.get('data'), dict):\n",
    "            print(f\"  Data keys: {list(sample['data'].keys())}\")\n",
    "            print(f\"  Data sample: {str(sample['data'])[:200]}\")\n",
    "else:\n",
    "    print(\"✓ No parse errors or exceptions found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract Cleaned Data\n",
    "\n",
    "Extract valid records from parsed data and convert to lightweight tuples for efficient processing. The cleaned data structure is:\n",
    "- **Format**: `(datetime_str, usage_kw_float, city_str, source_file_str)`\n",
    "- **Purpose**: Lightweight tuple format optimized for MapReduce operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sample Cleaned Data ===\n",
      "Data format: (datetime_str, usage_kw_float, city_str, source_file_str)\n",
      "  Record 1: ('2023-11-01 08:00:00', 0.33, 'Islamabad', 'islamabad_House41.csv')\n",
      "  Record 2: ('2023-11-01 08:01:00', 0.01, 'Islamabad', 'islamabad_House41.csv')\n",
      "  Record 3: ('2023-11-01 08:02:00', 0.1, 'Islamabad', 'islamabad_House41.csv')\n",
      "  Record 4: ('2023-11-01 08:03:00', 0.07, 'Islamabad', 'islamabad_House41.csv')\n",
      "  Record 5: ('2023-11-01 08:04:00', 0.14, 'Islamabad', 'islamabad_House41.csv')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:=====================================================>  (61 + 3) / 64]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total valid records available: 33,332,692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Display sample cleaned data and verify structure\n",
    "\n",
    "print(\"=== Sample Cleaned Data ===\")\n",
    "print(\"Data format: (datetime_str, usage_kw_float, city_str, source_file_str)\")\n",
    "sample_cleaned = clean_rdd.take(5)\n",
    "for i, rec in enumerate(sample_cleaned, 1):\n",
    "    print(f\"  Record {i}: {rec}\")\n",
    "    \n",
    "print(f\"\\nTotal valid records available: {clean_rdd.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Deduplication (MapReduce Reduce Phase)\n",
    "\n",
    "Remove duplicate records using MapReduce `reduceByKey` pattern. This is a classic Reduce operation:\n",
    "- **Key Generation**: Extract unique key from each record\n",
    "  - Primary: Household ID (from source_file) + datetime\n",
    "  - Fallback: City + datetime if household ID unavailable\n",
    "- **Reduce Operation**: Use `reduceByKey` to keep first occurrence when duplicates found\n",
    "- **Result**: Deduplicated RDD with unique records only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:=======================================================>(63 + 1) / 64]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplication complete\n",
      "Records before deduplication: 33,332,692\n",
      "Records after deduplication:  27,986,366\n",
      "Duplicates removed: 5,346,326 (16.04%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Deduplication using MapReduce pattern\n",
    "import re\n",
    "\n",
    "def extract_key_tuple(rec: Tuple[str, float, str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Extract a unique key for deduplication from a record tuple.\n",
    "    \n",
    "    Args:\n",
    "        rec: Tuple of (datetime_str, usage_kw, city, source_file)\n",
    "    \n",
    "    Returns:\n",
    "        Unique key string combining household ID (if available) or city with datetime\n",
    "    \"\"\"\n",
    "    dt, _usage, city, src = rec\n",
    "\n",
    "    # Extract household ID from source filename (e.g., \"lahore_House41.csv\" -> \"H041\")\n",
    "    hh = None\n",
    "    if src:\n",
    "        m = re.search(r'House(\\d+)', src, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            hh = f\"H{m.group(1).zfill(3)}\"\n",
    "\n",
    "    # Use household ID if available, otherwise fall back to city name\n",
    "    base = hh or (city.strip().lower() if city else \"unknown\")\n",
    "    return f\"{base}_{dt}\"\n",
    "\n",
    "# MapReduce deduplication: map records to (key, record) pairs, then reduceByKey to keep first occurrence\n",
    "keyed_rdd = clean_rdd.map(lambda rec: (extract_key_tuple(rec), rec))\n",
    "\n",
    "deduplicated_rdd = (keyed_rdd\n",
    "    .reduceByKey(lambda a, b: a)  # Keep first occurrence when duplicates found\n",
    "    .map(lambda kv: kv[1]))  # Extract record from (key, record) pair\n",
    "\n",
    "# Cache deduplicated RDD for subsequent operations\n",
    "deduplicated_rdd.cache()\n",
    "\n",
    "# Calculate deduplication statistics\n",
    "before_count = clean_rdd.count()\n",
    "after_count = deduplicated_rdd.count()\n",
    "duplicates_removed = before_count - after_count\n",
    "\n",
    "print(\"Deduplication complete\")\n",
    "print(f\"Records before deduplication: {before_count:,}\")\n",
    "print(f\"Records after deduplication:  {after_count:,}\")\n",
    "print(f\"Duplicates removed: {duplicates_removed:,} ({duplicates_removed/before_count*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Data Quality Validation (MapReduce Aggregations)\n",
    "\n",
    "Perform comprehensive data quality analysis using MapReduce aggregation patterns:\n",
    "- **Completeness Analysis**: Count null/empty values per field using `map` + `reduce`\n",
    "- **Range Validation**: Verify usage values are within valid bounds (0-50 kW)\n",
    "- **Statistical Summary**: Calculate min, max, average using MapReduce operations\n",
    "- **Distribution Analysis**: City distribution using `map` + `reduceByKey`\n",
    "- **Quality Score**: Overall data quality metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Quality Validation Report ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records after deduplication: 27,986,366\n",
      "\n",
      "1. Completeness Analysis (Null/Empty Values):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  datetime: 0 null/empty (0.00%)\n",
      "  usage_kw: 0 null/empty (0.00%)\n",
      "  city: 0 null/empty (0.00%)\n",
      "  source_file: 0 null/empty (0.00%)\n",
      "\n",
      "2. Range Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  usage_kw out of range (0-50 kW): 0 (0.00%)\n",
      "\n",
      "3. Statistical Summary (usage_kw):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Count: 27,986,366\n",
      "  Min: 0.000 kW\n",
      "  Max: 18.898 kW\n",
      "  Average: 0.729 kW\n",
      "  Total: 20,389,153.58 kW\n",
      "\n",
      "4. City Distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:===============================================>        (54 + 4) / 64]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Total cities: 6\n",
      "    Karachi: 5,222,447 records (18.66%)\n",
      "    Multan: 5,195,056 records (18.56%)\n",
      "    Peshawar: 5,022,626 records (17.95%)\n",
      "    Islamabad: 4,938,929 records (17.65%)\n",
      "    Lahore: 4,389,396 records (15.68%)\n",
      "    Skardu: 3,217,912 records (11.50%)\n",
      "\n",
      "5. Overall Data Quality Score:\n",
      "  Completeness: 25.00%\n",
      "  Validity: 100.00%\n",
      "  Overall Quality Score: 62.50%\n",
      "\n",
      "✓ Data quality validation complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Data Quality Validation using MapReduce\n",
    "from operator import add\n",
    "\n",
    "print(\"=== Data Quality Validation Report ===\\n\")\n",
    "\n",
    "# Get total record count\n",
    "total_records = deduplicated_rdd.count()\n",
    "print(f\"Total records after deduplication: {total_records:,}\")\n",
    "\n",
    "# 1. Null/Empty Value Checks\n",
    "print(\"\\n1. Completeness Analysis (Null/Empty Values):\")\n",
    "def check_field_completeness(rdd, field_index: int, field_name: str):\n",
    "    \"\"\"Check if field at index is None or empty string\"\"\"\n",
    "    null_or_empty = rdd.map(lambda rec: 1 if (len(rec) <= field_index or \n",
    "                                               rec[field_index] is None or \n",
    "                                               str(rec[field_index]).strip() == '') else 0) \\\n",
    "                           .reduce(add)\n",
    "    return null_or_empty\n",
    "\n",
    "datetime_nulls = check_field_completeness(deduplicated_rdd, 0, \"datetime\")\n",
    "usage_nulls = check_field_completeness(deduplicated_rdd, 1, \"usage_kw\")\n",
    "city_nulls = check_field_completeness(deduplicated_rdd, 2, \"city\")\n",
    "source_nulls = check_field_completeness(deduplicated_rdd, 3, \"source_file\")\n",
    "\n",
    "print(f\"  datetime: {datetime_nulls:,} null/empty ({datetime_nulls/total_records*100:.2f}%)\")\n",
    "print(f\"  usage_kw: {usage_nulls:,} null/empty ({usage_nulls/total_records*100:.2f}%)\")\n",
    "print(f\"  city: {city_nulls:,} null/empty ({city_nulls/total_records*100:.2f}%)\")\n",
    "print(f\"  source_file: {source_nulls:,} null/empty ({source_nulls/total_records*100:.2f}%)\")\n",
    "\n",
    "# 2. Range Validation\n",
    "print(\"\\n2. Range Validation:\")\n",
    "def count_out_of_range(rdd, min_val: float, max_val: float):\n",
    "    \"\"\"Count records where usage_kw is outside valid range\"\"\"\n",
    "    out_of_range = rdd.map(lambda rec: 1 if (len(rec) > 1 and \n",
    "                                             (rec[1] < min_val or rec[1] > max_val)) else 0) \\\n",
    "                         .reduce(add)\n",
    "    return out_of_range\n",
    "\n",
    "out_of_range_count = count_out_of_range(deduplicated_rdd, 0.0, 50.0)\n",
    "print(f\"  usage_kw out of range (0-50 kW): {out_of_range_count:,} ({out_of_range_count/total_records*100:.2f}%)\")\n",
    "\n",
    "# 3. Statistical Summary\n",
    "print(\"\\n3. Statistical Summary (usage_kw):\")\n",
    "usage_values = deduplicated_rdd.map(lambda rec: rec[1] if len(rec) > 1 else 0.0)\n",
    "\n",
    "# Calculate statistics using MapReduce\n",
    "usage_count = usage_values.count()\n",
    "usage_sum = usage_values.reduce(add)\n",
    "usage_min = usage_values.reduce(lambda a, b: min(a, b))\n",
    "usage_max = usage_values.reduce(lambda a, b: max(a, b))\n",
    "usage_avg = usage_sum / usage_count if usage_count > 0 else 0.0\n",
    "\n",
    "print(f\"  Count: {usage_count:,}\")\n",
    "print(f\"  Min: {usage_min:.3f} kW\")\n",
    "print(f\"  Max: {usage_max:.3f} kW\")\n",
    "print(f\"  Average: {usage_avg:.3f} kW\")\n",
    "print(f\"  Total: {usage_sum:,.2f} kW\")\n",
    "\n",
    "# 4. City Distribution\n",
    "print(\"\\n4. City Distribution:\")\n",
    "city_counts = (deduplicated_rdd\n",
    "    .map(lambda rec: (rec[2] if len(rec) > 2 else \"unknown\", 1))\n",
    "    .reduceByKey(add)\n",
    "    .collect())\n",
    "\n",
    "city_counts_sorted = sorted(city_counts, key=lambda x: x[1], reverse=True)\n",
    "print(f\"  Total cities: {len(city_counts_sorted)}\")\n",
    "for city, count in city_counts_sorted[:10]:  # Top 10 cities\n",
    "    print(f\"    {city}: {count:,} records ({count/total_records*100:.2f}%)\")\n",
    "\n",
    "# 5. Data Completeness Score\n",
    "print(\"\\n5. Overall Data Quality Score:\")\n",
    "completeness_score = ((total_records - datetime_nulls - usage_nulls - city_nulls - source_nulls) / \n",
    "                     (total_records * 4) * 100) if total_records > 0 else 0\n",
    "validity_score = ((total_records - out_of_range_count) / total_records * 100) if total_records > 0 else 0\n",
    "overall_score = (completeness_score + validity_score) / 2\n",
    "\n",
    "print(f\"  Completeness: {completeness_score:.2f}%\")\n",
    "print(f\"  Validity: {validity_score:.2f}%\")\n",
    "print(f\"  Overall Quality Score: {overall_score:.2f}%\")\n",
    "\n",
    "print(\"\\n✓ Data quality validation complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Silver Layer Output\n",
    "\n",
    "Save cleaned data to Silver layer storage and create DataFrame for final validation:\n",
    "- **Output Format**: JSON Lines (one JSON object per line)\n",
    "- **Storage Location**: `silver_data/` directory\n",
    "- **Rejected Records**: Save invalid records to `rejects/` for analysis\n",
    "- **DataFrame Conversion**: Create DataFrame for inspection (allowed per requirements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Final Data Preparation ===\n",
      "Records to save: 27,986,366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved cleaned data to: ../Part2_Data_Cleaning/silver_data\n",
      "✓ Total records saved: 27,986,366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved rejected records to: ../Part2_Data_Cleaning/rejects\n",
      "✓ Total rejected records: 2,401,839\n",
      "\n",
      "=== DataFrame View (Final Validation) ===\n",
      "Sample data (first 10 rows):\n",
      "+-------------------+--------+---------+---------------------+\n",
      "|datetime           |usage_kw|city     |source_file          |\n",
      "+-------------------+--------+---------+---------------------+\n",
      "|2023-11-05 08:04:00|0.52    |Islamabad|islamabad_House41.csv|\n",
      "|2023-11-12 10:48:00|0.61    |Islamabad|islamabad_House41.csv|\n",
      "|2023-11-17 08:06:00|0.0     |Islamabad|islamabad_House41.csv|\n",
      "|2023-11-17 08:09:00|0.0     |Islamabad|islamabad_House41.csv|\n",
      "|2023-11-17 18:44:00|0.6     |Islamabad|islamabad_House41.csv|\n",
      "|2023-11-24 00:09:00|0.94    |Islamabad|islamabad_House41.csv|\n",
      "|2023-11-27 02:44:00|1.03    |Islamabad|islamabad_House41.csv|\n",
      "|2023-11-29 08:09:00|1.25    |Islamabad|islamabad_House41.csv|\n",
      "|2023-12-02 10:41:00|0.79    |Islamabad|islamabad_House41.csv|\n",
      "|2023-12-02 10:47:00|1.15    |Islamabad|islamabad_House41.csv|\n",
      "+-------------------+--------+---------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Schema:\n",
      "root\n",
      " |-- datetime: string (nullable = true)\n",
      " |-- usage_kw: double (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- source_file: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:=======================================================>(63 + 1) / 64]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total rows in DataFrame: 27,986,366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Final Data Preparation: Convert to Silver Layer Format\n",
    "\n",
    "# deduplicated_rdd contains: (datetime_str, usage_kw_float, city_str, source_file_str)\n",
    "final_clean_rdd = deduplicated_rdd\n",
    "\n",
    "# JSON line serializer for tuples\n",
    "def tuple_to_json(rec: Tuple[str, float, str, str]) -> str:\n",
    "    \"\"\"Convert tuple to JSON string for Silver layer output\"\"\"\n",
    "    dt_str, usage, city, src = rec\n",
    "    return json.dumps({\n",
    "        \"datetime\": dt_str,\n",
    "        \"usage_kw\": float(usage),\n",
    "        \"city\": city,\n",
    "        \"source_file\": src\n",
    "    })\n",
    "\n",
    "# Output directories\n",
    "import os, shutil\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def _remove_directory_if_exists(path: str):\n",
    "    \"\"\"Remove directory if it exists, handling both local paths and file:// URIs.\"\"\"\n",
    "    if path.startswith(\"file:\"):\n",
    "        path = urlparse(path).path\n",
    "    if not os.path.isabs(path):\n",
    "        path = os.path.abspath(path)\n",
    "    shutil.rmtree(path, ignore_errors=True)\n",
    "\n",
    "silver_out_dir = \"../Part2_Data_Cleaning/silver_data\"\n",
    "rejects_out_dir = \"../Part2_Data_Cleaning/rejects\"\n",
    "\n",
    "# Prepare output directories by removing existing content (RDD API requires empty directories)\n",
    "_remove_directory_if_exists(silver_out_dir)\n",
    "_remove_directory_if_exists(rejects_out_dir)\n",
    "\n",
    "# Save cleaned data to Silver layer\n",
    "record_count = final_clean_rdd.count()\n",
    "print(f\"=== Final Data Preparation ===\")\n",
    "print(f\"Records to save: {record_count:,}\")\n",
    "\n",
    "if record_count > 0:\n",
    "    # Save cleaned data\n",
    "    (final_clean_rdd\n",
    "        .map(tuple_to_json)\n",
    "        .coalesce(8)\n",
    "        .saveAsTextFile(silver_out_dir))\n",
    "    \n",
    "    print(f\"✓ Saved cleaned data to: {silver_out_dir}\")\n",
    "    print(f\"✓ Total records saved: {record_count:,}\")\n",
    "    \n",
    "    # Save rejected records (for analysis)\n",
    "    rejects_rdd = (parsed_rdd\n",
    "        .filter(lambda x: x['status'] in ('invalid', 'parse_error', 'error'))\n",
    "        .map(lambda x: json.dumps({\n",
    "            \"status\": x['status'], \n",
    "            \"errors\": x['errors']\n",
    "        })))\n",
    "    \n",
    "    reject_count = rejects_rdd.count()\n",
    "    if reject_count > 0:\n",
    "        (rejects_rdd\n",
    "            .coalesce(2)\n",
    "            .saveAsTextFile(rejects_out_dir))\n",
    "        print(f\"✓ Saved rejected records to: {rejects_out_dir}\")\n",
    "        print(f\"✓ Total rejected records: {reject_count:,}\")\n",
    "    \n",
    "    # Create DataFrame for final inspection and validation\n",
    "    print(\"\\n=== DataFrame View (Final Validation) ===\")\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"datetime\", StringType(), True),\n",
    "        StructField(\"usage_kw\", DoubleType(), True),\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"source_file\", StringType(), True),\n",
    "    ])\n",
    "    \n",
    "    # Convert RDD to DataFrame\n",
    "    df_ready = final_clean_rdd.map(lambda r: (r[0], float(r[1]), r[2], r[3]))\n",
    "    final_df = spark.createDataFrame(df_ready, schema=schema)\n",
    "    \n",
    "    print(\"Sample data (first 10 rows):\")\n",
    "    final_df.show(10, truncate=False)\n",
    "    print(\"\\nSchema:\")\n",
    "    final_df.printSchema()\n",
    "    print(f\"\\nTotal rows in DataFrame: {final_df.count():,}\")\n",
    "else:\n",
    "    print(\"⚠️  No cleaned records to save!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Performance Profiling and Analysis\n",
    "\n",
    "Measure and analyze performance of MapReduce operations to identify bottlenecks:\n",
    "- **Execution Time**: Time taken for each major operation (loading, parsing, deduplication, etc.)\n",
    "- **Throughput Analysis**: Records processed per second for each stage\n",
    "- **Partition Analysis**: Partition count and distribution across operations\n",
    "- **Data Reduction Metrics**: Records removed at each stage\n",
    "- **Resource Usage**: Memory and storage insights (via Spark UI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Performance Profiling ===\n",
      "Measuring execution time for each major operation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Data Loading:\n",
      "  Time: 13.194 seconds\n",
      "  Records: 35,734,531\n",
      "  Throughput: 2,708,433 records/second\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Parsing & Validation:\n",
      "  Time: 9.051 seconds\n",
      "  Records: 35,734,531\n",
      "  Throughput: 3,948,119 records/second\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Filtering Valid Records:\n",
      "  Time: 11.448 seconds\n",
      "  Records: 33,332,692\n",
      "  Throughput: 2,911,684 records/second\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Deduplication:\n",
      "  Time: 4.249 seconds\n",
      "  Records: 27,986,366\n",
      "  Throughput: 6,586,241 records/second\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Final Output:\n",
      "  Time: 4.189 seconds\n",
      "  Records: 27,986,366\n",
      "  Throughput: 6,681,001 records/second\n",
      "\n",
      "Total Processing Time: 42.131 seconds (0.70 minutes)\n",
      "\n",
      "=== Throughput Summary ===\n",
      "1. Data Loading: 2,708,433 records/second\n",
      "2. Parsing & Validation: 3,948,119 records/second\n",
      "3. Filtering Valid Records: 2,911,684 records/second\n",
      "4. Deduplication: 6,586,241 records/second\n",
      "5. Final Output: 6,681,001 records/second\n",
      "\n",
      "=== Partition Analysis ===\n",
      "Raw data partitions: 64\n",
      "Parsed data partitions: 64\n",
      "Cleaned data partitions: 64\n",
      "Deduplicated data partitions: 64\n",
      "Default parallelism: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 88:==================================================>     (58 + 4) / 64]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal partitions (records/10k): 2798\n",
      "\n",
      "=== Data Reduction Analysis ===\n",
      "Raw records: 35,734,531\n",
      "Final cleaned records: 27,986,366\n",
      "Records removed: 7,748,165 (21.68%)\n",
      "Data retention: 78.32%\n",
      "\n",
      "=== Resource Usage ===\n",
      "For detailed metrics, check Spark UI:\n",
      "  - Application UI: http://localhost:4040\n",
      "  - Jobs: http://localhost:4040/jobs/\n",
      "  - Stages: http://localhost:4040/stages/\n",
      "  - Storage: http://localhost:4040/storage/\n",
      "\n",
      "Key metrics to monitor:\n",
      "  - Shuffle read/write sizes\n",
      "  - Task execution times\n",
      "  - Memory usage\n",
      "  - Data skew indicators\n",
      "\n",
      "Performance profiling complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Performance Profiling and Analysis\n",
    "import time\n",
    "\n",
    "print(\"=== Performance Profiling ===\")\n",
    "print(\"Measuring execution time for each major operation...\\n\")\n",
    "\n",
    "# Measure execution time for each major operation\n",
    "operations = {\n",
    "    '1. Data Loading': lambda: raw_rdd.count(),\n",
    "    '2. Parsing & Validation': lambda: parsed_rdd.count(),\n",
    "    '3. Filtering Valid Records': lambda: clean_rdd.count(),\n",
    "    '4. Deduplication': lambda: deduplicated_rdd.count(),\n",
    "    '5. Final Output': lambda: final_clean_rdd.count()\n",
    "}\n",
    "\n",
    "profiling_results = {}\n",
    "for op_name, op_func in operations.items():\n",
    "    start_time = time.time()\n",
    "    result = op_func()\n",
    "    elapsed = time.time() - start_time\n",
    "    profiling_results[op_name] = {\n",
    "        'time': elapsed,\n",
    "        'records': result\n",
    "    }\n",
    "    print(f\"{op_name}:\")\n",
    "    print(f\"  Time: {elapsed:.3f} seconds\")\n",
    "    print(f\"  Records: {result:,}\")\n",
    "    if result > 0:\n",
    "        throughput = result / elapsed\n",
    "        print(f\"  Throughput: {throughput:,.0f} records/second\")\n",
    "    print()\n",
    "\n",
    "# Calculate total processing time\n",
    "total_time = sum(r['time'] for r in profiling_results.values())\n",
    "print(f\"Total Processing Time: {total_time:.3f} seconds ({total_time/60:.2f} minutes)\")\n",
    "\n",
    "# Throughput Summary\n",
    "print(\"\\n=== Throughput Summary ===\")\n",
    "for op_name, results in profiling_results.items():\n",
    "    if results['records'] > 0:\n",
    "        throughput = results['records'] / results['time']\n",
    "        print(f\"{op_name}: {throughput:,.0f} records/second\")\n",
    "\n",
    "# Partition Analysis\n",
    "print(\"\\n=== Partition Analysis ===\")\n",
    "print(f\"Raw data partitions: {raw_rdd.getNumPartitions()}\")\n",
    "print(f\"Parsed data partitions: {parsed_rdd.getNumPartitions()}\")\n",
    "print(f\"Cleaned data partitions: {clean_rdd.getNumPartitions()}\")\n",
    "print(f\"Deduplicated data partitions: {deduplicated_rdd.getNumPartitions()}\")\n",
    "print(f\"Default parallelism: {sc.defaultParallelism}\")\n",
    "print(f\"Optimal partitions (records/10k): {max(sc.defaultParallelism, final_clean_rdd.count() // 10000)}\")\n",
    "\n",
    "# Data Reduction Analysis\n",
    "print(\"\\n=== Data Reduction Analysis ===\")\n",
    "raw_count = profiling_results['1. Data Loading']['records']\n",
    "final_count = profiling_results['5. Final Output']['records']\n",
    "reduction_pct = ((raw_count - final_count) / raw_count * 100) if raw_count > 0 else 0\n",
    "print(f\"Raw records: {raw_count:,}\")\n",
    "print(f\"Final cleaned records: {final_count:,}\")\n",
    "print(f\"Records removed: {raw_count - final_count:,} ({reduction_pct:.2f}%)\")\n",
    "print(f\"Data retention: {final_count/raw_count*100:.2f}%\")\n",
    "\n",
    "# Resource Usage\n",
    "print(\"\\n=== Resource Usage ===\")\n",
    "print(\"For detailed metrics, check Spark UI:\")\n",
    "print(\"  - Application UI: http://localhost:4040\")\n",
    "print(\"  - Jobs: http://localhost:4040/jobs/\")\n",
    "print(\"  - Stages: http://localhost:4040/stages/\")\n",
    "print(\"  - Storage: http://localhost:4040/storage/\")\n",
    "print(\"\\nKey metrics to monitor:\")\n",
    "print(\"  - Shuffle read/write sizes\")\n",
    "print(\"  - Task execution times\")\n",
    "print(\"  - Memory usage\")\n",
    "print(\"  - Data skew indicators\")\n",
    "\n",
    "print(\"\\nPerformance profiling complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Optimization Strategies and Tuning Recommendations\n",
    "\n",
    "Document optimization strategies based on profiling results:\n",
    "- **Partitioning Optimization**: Optimal partition count based on data size\n",
    "- **Caching Strategy**: When and how to cache RDDs for reuse\n",
    "- **Broadcast Variables**: Using broadcast for small lookup data\n",
    "- **Data Skew Handling**: Techniques for handling uneven data distribution\n",
    "- **Serialization**: KryoSerializer configuration and benefits\n",
    "- **Memory Configuration**: Tuning memory settings for large datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Optimization Strategies ===\n",
      "\n",
      "1. Partitioning Optimization:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 91:==================================================>     (58 + 4) / 64]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Current partitions: 64\n",
      "  Recommended partitions: 2798\n",
      "  Strategy: Repartition to 2798 partitions\n",
      "  Code: final_clean_rdd.repartition(optimal_partitions) or .coalesce(optimal_partitions)\n",
      "\n",
      "2. Caching Strategy:\n",
      "  - Cache RDDs that are used multiple times (e.g., parsed_rdd, clean_rdd)\n",
      "  - Use MEMORY_AND_DISK storage level for large datasets\n",
      "  - Unpersist cached RDDs when no longer needed\n",
      "  Code: rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
      "\n",
      "3. Broadcast Variables:\n",
      "  - Use for small lookup tables or reference data\n",
      "  - Reduces network overhead in map operations\n",
      "  Code: broadcast_var = sc.broadcast(lookup_dict)\n",
      "\n",
      "4. Data Skew Handling:\n",
      "  - Monitor task execution times in Spark UI\n",
      "  - If skew detected, use salting technique or repartition\n",
      "  - Consider custom partitioner for reduce operations\n",
      "\n",
      "5. Serialization:\n",
      "  - Current: KryoSerializer (configured)\n",
      "  - Benefits: Faster serialization, smaller data size\n",
      "  - Register custom classes if needed: spark.conf.set('spark.kryo.classesToRegister', ...)\n",
      "\n",
      "6. Memory Configuration:\n",
      "  - Monitor memory usage in Spark UI\n",
      "  - Adjust spark.executor.memory if needed\n",
      "  - Consider spark.memory.fraction and spark.memory.storageFraction\n",
      "\n",
      "=== Tuning Parameters Applied ===\n",
      "✓ spark.sql.adaptive.enabled = true\n",
      "✓ spark.sql.adaptive.coalescePartitions.enabled = true\n",
      "✓ spark.serializer = KryoSerializer\n",
      "✓ Log level set to WARN (reduced verbosity)\n",
      "\n",
      "=== Performance Bottlenecks Identified ===\n",
      "Slowest operation: 1. Data Loading (13.194s)\n",
      "Recommendation: Focus optimization efforts on 1. Data Loading\n",
      "\n",
      "⚠️  High partition count detected - consider coalescing\n",
      "\n",
      "=== Next Steps for Optimization ===\n",
      "1. Run with optimized partition count and measure improvement\n",
      "2. Add caching for frequently accessed RDDs\n",
      "3. Monitor Spark UI during execution\n",
      "4. Adjust memory settings if out-of-memory errors occur\n",
      "5. Consider checkpointing for long lineage chains\n",
      "\n",
      "✓ Optimization analysis complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Optimization Strategies and Tuning Recommendations\n",
    "\n",
    "print(\"=== Optimization Strategies ===\")\n",
    "print(\"\\n1. Partitioning Optimization:\")\n",
    "current_partitions = final_clean_rdd.getNumPartitions()\n",
    "optimal_partitions = max(sc.defaultParallelism * 2, final_clean_rdd.count() // 10000)\n",
    "print(f\"  Current partitions: {current_partitions}\")\n",
    "print(f\"  Recommended partitions: {optimal_partitions}\")\n",
    "print(f\"  Strategy: {'Repartition' if current_partitions < optimal_partitions else 'Coalesce'} to {optimal_partitions} partitions\")\n",
    "print(\"  Code: final_clean_rdd.repartition(optimal_partitions) or .coalesce(optimal_partitions)\")\n",
    "\n",
    "print(\"\\n2. Caching Strategy:\")\n",
    "print(\"  - Cache RDDs that are used multiple times (e.g., parsed_rdd, clean_rdd)\")\n",
    "print(\"  - Use MEMORY_AND_DISK storage level for large datasets\")\n",
    "print(\"  - Unpersist cached RDDs when no longer needed\")\n",
    "print(\"  Code: rdd.persist(StorageLevel.MEMORY_AND_DISK)\")\n",
    "\n",
    "print(\"\\n3. Broadcast Variables:\")\n",
    "print(\"  - Use for small lookup tables or reference data\")\n",
    "print(\"  - Reduces network overhead in map operations\")\n",
    "print(\"  Code: broadcast_var = sc.broadcast(lookup_dict)\")\n",
    "\n",
    "print(\"\\n4. Data Skew Handling:\")\n",
    "print(\"  - Monitor task execution times in Spark UI\")\n",
    "print(\"  - If skew detected, use salting technique or repartition\")\n",
    "print(\"  - Consider custom partitioner for reduce operations\")\n",
    "\n",
    "print(\"\\n5. Serialization:\")\n",
    "print(\"  - Current: KryoSerializer (configured)\")\n",
    "print(\"  - Benefits: Faster serialization, smaller data size\")\n",
    "print(\"  - Register custom classes if needed: spark.conf.set('spark.kryo.classesToRegister', ...)\")\n",
    "\n",
    "print(\"\\n6. Memory Configuration:\")\n",
    "print(\"  - Monitor memory usage in Spark UI\")\n",
    "print(\"  - Adjust spark.executor.memory if needed\")\n",
    "print(\"  - Consider spark.memory.fraction and spark.memory.storageFraction\")\n",
    "\n",
    "print(\"\\n=== Tuning Parameters Applied ===\")\n",
    "print(\"✓ spark.sql.adaptive.enabled = true\")\n",
    "print(\"✓ spark.sql.adaptive.coalescePartitions.enabled = true\")\n",
    "print(\"✓ spark.serializer = KryoSerializer\")\n",
    "print(\"✓ Log level set to WARN (reduced verbosity)\")\n",
    "\n",
    "print(\"\\n=== Performance Bottlenecks Identified ===\")\n",
    "# Analyze profiling results\n",
    "slowest_op = max(profiling_results.items(), key=lambda x: x[1]['time'])\n",
    "print(f\"Slowest operation: {slowest_op[0]} ({slowest_op[1]['time']:.3f}s)\")\n",
    "print(f\"Recommendation: Focus optimization efforts on {slowest_op[0]}\")\n",
    "\n",
    "# Check for data skew (if partitions vary significantly)\n",
    "if final_clean_rdd.getNumPartitions() > sc.defaultParallelism * 4:\n",
    "    print(\"\\n⚠️  High partition count detected - consider coalescing\")\n",
    "elif final_clean_rdd.getNumPartitions() < sc.defaultParallelism:\n",
    "    print(\"\\n⚠️  Low partition count detected - consider repartitioning\")\n",
    "\n",
    "print(\"\\n=== Next Steps for Optimization ===\")\n",
    "print(\"1. Run with optimized partition count and measure improvement\")\n",
    "print(\"2. Add caching for frequently accessed RDDs\")\n",
    "print(\"3. Monitor Spark UI during execution\")\n",
    "print(\"4. Adjust memory settings if out-of-memory errors occur\")\n",
    "print(\"5. Consider checkpointing for long lineage chains\")\n",
    "\n",
    "print(\"\\n✓ Optimization analysis complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark session stopped\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "parsed_rdd.unpersist()\n",
    "clean_rdd.unpersist()\n",
    "deduplicated_rdd.unpersist()\n",
    "\n",
    "spark.stop()\n",
    "print(\"✓ Spark session stopped\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
